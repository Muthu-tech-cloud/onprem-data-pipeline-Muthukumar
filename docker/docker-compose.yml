version: '3.8'

services:
  zookeeper:
    image: bitnami/zookeeper:latest
    ports:
      - "2181:2181"

  kafka:
    image: bitnami/kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENERS: PLAINTEXT://0.0.0.0:9092
    depends_on:
      - zookeeper

  mysql:
    image: mysql:8.0
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: sensordb
    ports:
      - "3306:3306"

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    environment:
      - HIVE_METASTORE_DB_TYPE=postgres
    ports:
      - "10000:10000"
      - "9083:9083"

  spark:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ../spark_jobs:/opt/spark_jobs

  airflow:
    image: apache/airflow:2.8.0
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ../dags:/opt/airflow/dags
      - ../ingestion:/opt/airflow/ingestion
    ports:
      - "8081:8080"
    command: >
      bash -c "airflow db init &&
               airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com &&
               airflow webserver & airflow scheduler"
